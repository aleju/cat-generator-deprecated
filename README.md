Work in progress

# About

This script generates new images of cats using the technique of generative adversarial networks (GAN), as described in the paper by [Ian J. Goodfellow](http://arxiv.org/abs/1406.2661).
It is based on code from facebook's [eyescream project](https://github.com/facebook/eyescream).

The basic principle of GANs is to train two networks in a kind of pupil teacher relationship.
The pupil is called G (generator) and the teacher D (discriminator).
G generates new images, D tells fake images from real images apart.
They are both jointly trained, i.e. G's training objective is to trick D into believing that its outputs are real images, while D is trained to spot G's fakes.
At the end you (hopefully) get a G that produces beautiful images matching your training dataset.


# Images

Full sized examples:

- big images


Output from 16x16 network:

- 512 16x16 images


Training progress of 16x16 network:

- gif progress per epoch


16x16 images with nearest neighbour from training set:

- gif


Color examples:

- color: end result, random images + training set comparison


Painter style images:
- gif


# Requirements

* [Torch](http://torch.ch/) with the following libraries (most of them are probably already installed by default):
  * `nn` (`luarocks install nn`)
  * `paths` (`luarocks install paths`)
  * `image` (`luarocks install image`)
  * `optim` (`luarocks install optim`)
  * `cutorch` (`luarocks install cutorch`)
  * `cunn` (`luarocks install cunn`)
  * `dpnn` (`luarocks install dpnn`)
  * `stn` [see here](https://github.com/qassemoquab/stnbhwd)
* Python 2.7 (only tested with that version)
  * scipy
  * numpy
  * scikit-image
* [display](https://github.com/szym/display)
* [10k cats dataset](https://web.archive.org/web/20150520175645/http://137.189.35.203/WebUI/CatDatabase/catData.html)
* CUDA capable GPU (ca. 3GB memory or more) with cudnn3

- todo all requirements?

# Usage

* Install all requirements.
* Download and extract the [10k cats dataset](https://web.archive.org/web/20150520175645/http://137.189.35.203/WebUI/CatDatabase/catData.html) into a directory, e.g. `/foo/bar`. That folder should then contain the subfolders `CAT_00` to `CAT_06`.
* Clone the repository.
* Switch to the repository's subdirectory `dataset` via `cd dataset` and convert your downloaded cat images into a normalized and augmented set of cat faces with `python generate_dataset.py --path="/foo/bar"`. This may take a good two hours or so to run through, as it performs lots of augmentations.
* Start display with `~/.display/run.js &`
* Open `http://localhost:8000/` in your browser (plotting interface by display).
* Train V for 50 epochs with `th train_v.lua --grayscale`. (Wait for a `saved to directory` message, then stop manually.) (~10min)
* Pretrain G for 20 epochs with `th pretrain_g.lua --grayscale`. (Wait for a `saved to directory` message, then stop manually.) (This step can be skipped.) (~10min)
* Train a 16x16 base network with `th train.lua --grayscale --D_iterations=2`. Takes about 150 epochs to get good. (~3h)
* Train a 16 to 22 refiner with `th train_c2f.lua --grayscale --coarseSize=16 --fineSize=22`. Takes maybe 100 epochs or so to get good. (~3h)
* Train a 22 to 32 refiner with `th train_c2f.lua --grayscale --coarseSize=22 --fineSize=32`. Takes maybe 100 epochs or so to get good. (~3h)
* Sample images with `th sample.lua`.

Results seem to not be identically reproduceable between runs (I don't know why), so you might try restarting if things go wrong.


# V

V (Validator) is intended to be a half-decent replacement of a validation scores, which you don't have in GANs. V's architecture is - similar to D - a convolutional neural network.
Just like D, V creates fake/real judgements for images, i. e. it rates how fake images look. V gets fed images generated by G and rates them. The mean of that rating can be used
as the mentioned validation score.
V is trained once before the 16x16 run. During that training, V sees real images from the dataset as well as synthetically generated images. The methods to generate the synthetic
images are roughly:
* Random mixing of two images.
* Random warping of an image (i. e. move parts of the image around, causing distortions).
* Random stamping of an image (i. e. replace parts of the image by parts from somewhere else in the image).
* Randomly through random pixel values together (with some gaussian blurring technique, so that its not just gaussian noise).

The rating by V was sometimes quite off and overall noticeably worse than a good validation set with an accuracy/loss value.
However, more often than not seemed to at least roughly resemble the real image quality.


# Architecture


- current architecture D
- current architecture G
- current architecture V

- something about d iterations


# Training procedure



`Adam` is used as the optimizer during training.
