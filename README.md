# About

This script generates new images of cats using the technique of generative adversarial networks (GAN), as described in [the paper by Ian J. Goodfellow et al.](http://arxiv.org/abs/1406.2661).
The images are enhanced with the [laplacian pyramid technique](http://arxiv.org/abs/1506.05751) from Denton and ([Soumith](https://github.com/soumith)) Chintala et. al.
Most of the code is based on facebook's [eyescream project](https://github.com/facebook/eyescream).


# Images

Full sized examples:

![Full sized example images](images/examples_full_sized.png?raw=true "Full sized example images")
*Full sized images, generated by the chain of networks*


![512 generated 16x16 images](images/examples_512_16x16.png?raw=true "512 generated 16x16 images")
*512 16x16 cat images, randomly generated by G (base)*


![Training progess by epoch](images/examples_progress_by_epoch.png?raw=true "Training progress by epoch")
*Training progress by epoch (16x16 generator, starts pretrained at first epoch)*


![Nearest neaghbours of generated 16x16 images](images/examples_nearest_neighbours.png?raw=true "Nearest neighbours of generated 16x16 images")
*Nearest neighbours in the training set of 32 random generated 16x16 images (2-Norm)*


![Color 16x16 images](images/examples_color_images.png?raw=true "Color 16x16 images")
*Generated 16x16 color images after application of G2. Note that the network was not fine-tuned for color images, only for grayscale.*


TODO Painter style images


# Background Knowledge

The basic principle of GANs is to train two networks in a kind of forger-police-relationship.
The forger is called G (generator) and the police D (discriminator).
It's D's job to take a look at an image and estimate whether it a fake or an original image (from the training set).
Naturally it's G's job to generate images that trick D into believing that they are from the training set.
With a large enough training set and some regularization strategies, D cannot just memorize the training set.
As a result, D must learn the general rules that govern the look of images from the training set (i.e. a generalizing function).
Similarly, G must learn how to "paint" new images that look like the ones in the training set, otherwise it would not be able to trick D.

The previously mentioned laplacian pyramid technique for GANs is pretty straight-forward:
Instead of training G and D on full-sized images (e.g. 64x64 pixels) you train them on small ones (e.g. 8x8 pixels).
Afterwards you increase the size of the generated images (by G) in multiple steps to the final size, e.g. from 8x8 to 16x16 to 32x32 to 64x64.
For each of these steps you train another pair of G and D, but in case of these upscaling steps they are trained to learn good refinements of the upscaled (and hence blurry) images.
That means that D gets fed blurry upscaled images and their refinements (to sharpen them). Sometimes they are the real refinements, sometimes they have been generated by G.
Again, G must learn to generate good refinements and D must learn what good refinements (for specific images) look like.
The image below (taken from the paper) shows the process (they start with the full sized images, the one on the far right could be generated by a GAN).
Note that this training methodology ist similar to how one would naturally paint images: You start with a rough sketch (low resolution image) and then progressively add more and more details (increases in resolution).


# Requirements

* [Torch](http://torch.ch/) with the following libraries (most of them are probably already installed by default):
  * `nn` (`luarocks install nn`)
  * `paths` (`luarocks install paths`)
  * `image` (`luarocks install image`)
  * `optim` (`luarocks install optim`)
  * `cutorch` (`luarocks install cutorch`)
  * `cunn` (`luarocks install cunn`)
  * `dpnn` (`luarocks install dpnn`)
  * `stn` ([see here](https://github.com/qassemoquab/stnbhwd))
* Python 2.7 (only tested with that version)
  * scipy
  * numpy
  * scikit-image
* [display](https://github.com/szym/display)
* [10k cats dataset](https://web.archive.org/web/20150520175645/http://137.189.35.203/WebUI/CatDatabase/catData.html)
* CUDA capable GPU (~3GB memory or more) with cudnn3

- todo all requirements?

# Usage

* Install all requirements.
* Download and extract the [10k cats dataset](https://web.archive.org/web/20150520175645/http://137.189.35.203/WebUI/CatDatabase/catData.html) into a directory, e.g. `/foo/bar`. That folder should then contain the subfolders `CAT_00` to `CAT_06`.
* Clone the repository.
* Switch to the repository's subdirectory `dataset` via `cd dataset` and convert your downloaded cat images into a normalized and augmented set of ~100k cat faces with `python generate_dataset.py --path="/foo/bar"`. This may take a good two hours or so to run through, as it performs lots of augmentations.
* Start display with `~/.display/run.js &`
* Open `http://localhost:8000/` in your browser (plotting interface by display).
* Train V for 50 epochs with `th train_v.lua --grayscale`. (Wait for a `saving network to <path>` message, then stop manually.) (~10min)
* Pretrain G for 20 epochs with `th pretrain_g.lua --grayscale`. (Wait for a `saving network to <path>` message, then stop manually.) (This step can be skipped.) (~10min)
* Train a 16x16 base network with `th train.lua --grayscale --D_iterations=2`. Takes about 150 epochs to get good. (~3h)
* Train a 16 to 22 refiner with `th train_c2f.lua --grayscale --coarseSize=16 --fineSize=22 --D_iterations=2`. Takes maybe 100 epochs or so to get good. More epochs is better. Performance between epochs seems to vary quite a lot, so you have to take care of stopping on a good save. (~3h+)
* Train a 22 to 32 refiner with `th train_c2f.lua --grayscale --coarseSize=22 --fineSize=32`. Takes maybe 100 epochs or so to get good. More epochs is better. Performance between epochs seems to vary quite a lot, so you have to take care of stopping on a good save. (~3h+)
* Sample images with `th sample.lua --grayscale`.

Results seem to not be identically reproduceable between runs (I currently don't know why), so you might try restarting if things go wrong.

You can run `th train2.lua --grayscale` to generate a pair of G with G2. G2 is a refiner of G's results. They are jointly trained under the same D.


# V

V (Validator) is intended to be a half-decent replacement of validation scores, which you don't have in GANs. V's architecture is - similarly to D - a convolutional neural network.
Just like D, V creates fake/real judgements for images, i. e. it rates how fake images look. V gets fed images generated by G and rates them. The mean of that rating can be used
as the mentioned validation score.
V is trained once before the 16x16 run. During that training, V sees real images from the dataset as well as synthetically generated images. The methods to generate the synthetic
images are roughly:
* Random mixing of two images.
* Random warping of an image (i. e. move parts of the image around, causing distortions).
* Random stamping of an image (i. e. replace parts of the image by parts from somewhere else in the image).
* Randomly throw random pixel values together (with some gaussian blurring technique, so that its not just gaussian noise).

These techniques are then sometimes combined with each other, e. g. one image is modified by warping, another by stamping and then both are mixed into one final synthetic image.

The rating by V was sometimes quite off and overall noticeably worse than a good validation set with an accuracy/loss value.
However, more often than not it seemed to at least roughly resemble the real image quality.


# Architecture

All networks are optimized for grayscale image generation.
That was mainly the case to simplify the problem and reduce the computational burden.
Most of the activations were PReLUs, because they perform better than ReLUs in my experience.
LeakyReLUs lead to blowups.

## G (base / 16x16 images)

The base G is a very small network with just one hidden layer.
I tried many times using multiple hidden layers, but that just resulted in frequent blowups or inferior results.
Choosing other sizes for the hidden layer also lead to blowups or worse results.

![Architecture of G](diagrams/G.png?raw=true "Architecture of G")

## G2 (base / 16x16 images)

G2 is a small upsampling network. It is similar to the coarse to fine Gs, but has less capacity.
Notably, it outputs a full new image instead of just a difference (as the coarse to fine Gs do).
Using G2 during training seemed to worsen the results of the 16x16 G.
G2 would always end up just denoising the image instead of adding new details.
Sometimes that looks good, but usually the loss of G's quality outweighed G2.
I also tried normal networks with hidden neurons. Sometimes they seemed to add details, sometimes they made things worse.
Might be a good route for further exploration.

* SpatialConvolutionUpsample, 32 times 3x3
* PReLU
* SpatialConvolutionUpsample, 64 times 5x5
* PReLU
* SpatialConvolutionUpsample, 64 times 5x5
* Sigmoid


## D (base / 16x16 images)

The base D is a standard convolutional network with multiple branches.
It uses a spatial transformer at the start to remove rotations.
Three of the four branches also have spatial transformers, so that they can learn to focus on specific areas of the image.
The fourth branch is intended to analyze the whole image.

The transformers a not strictly necessary. Networks without them only seemed to be marginally worse (might have been subjective as GANs don't have an objective score).
The same is the case for networks with pooling layers. Batch Normalization however couldn't be added.
It lead to D having basically immediately 100% accuracy, leaving G no time to learn anything.

![Architecture of D](diagrams/D.png?raw=true "Architecture of D")

## G and D (coarse to fine / laplacian pyramid)

Both coarse to fine steps use basically the same networks. They are mostly taken from the eyescream project.
(I did not spend that much time optimizing these steps, different networks would have most likely been better.)

### G 22x22 and 32x32

* Join noise layer and image layer to two-channel image
* Spatial convolutional upsampling, 64 times 3x3
* PReLU
* Spatial convolutional upsampling, 512 times 7x7
* PReLU
* Spatial convolutional upsampling, 1 time 5x5 (i.e. from 512 channels down to a grayscale image)
* No activation / linear

### D 22x22

* Convolution, 64 times 3x3, no padding (decreases image size to 20x20)
* PReLU
* Convolution, 256 times 5x5
* PReLU
* Max Pooling 2x2
* Convolution, 1024 times 3x3
* PReLU
* Max Pooling 2x2
* Dropout
* Linear/Fully connected layer from `1024*5*5 = 25,600` to `1`
* Sigmoid

### D 32x32

* Convolution, 64 times 3x3
* PReLU
* Convolution, 256 times 5x5
* PReLU
* Max Pooling 2x2
* Convolution, 1024 times 3x3
* PReLU
* Max Pooling 2x2
* Dropout
* Linear/Fully connected layer from `1024*8*8 = 65,536` to `1`
* Sigmoid

## V

The validator is a standard convolutional network.
* Convolution 128 times 3x3, LeakyReLU
* Convolution 128 times 3x3, LeakyReLU
* Spatial Batch Normalization (before relu)
* Dropout
* Convolution 256 times 3x3, LeakyReLU
* Convolution 256 times 3x3, LeakyReLU
* Spatial Batch Normalization (before relu)
* Max Pooling (2x2)
* Spatial Dropout
* Linear/Fully Connected Layer from `256*8*8 = 16,384` to `1024`
* Batch Normalization
* LeakyReLU
* Dropout
* Linear/Fully Connected Layer from `1024` to `1024`
* Batch Normalization
* LeakyReLU
* Dropout
* Linar/Fully Connected Layer from `1024` to `2`
* Softmax

(A 1-neuron sigmoid output would have probably been more logical.)


# Preprocessing, training and sampling procedure

As a preprocessing step, the faces must be extracted from the 10k cat images dataset.
That dataset contains facial keypoints for each image (ears, eyes, nose), so that extracting isn't too hard.
Each of the faces gets rotated so that the eyeline is parallel to the x axis (i.e. rotations are removed).
That was necessary as many cat images tend to be heavily rotated, making the learning task for G significantly harder.
After that normalizing step, the images are augmented by introducing (now small) rotations, translations, scalings, brightness changes and adding minor gaussian noise.
That bloats up the total size of the dataset from 10k to roughly 100k images.

Training starts with G and D in the standard GAN setting. G generates images, D rates them.
In the case of `train2.lua` (not used for most of the images at the top of this readme!), a second G (G2) is also trained at the same time.
G2 takes images from G and applies a few SpatialUpsampling layers to them.
Then both G and G2 feed their images into D, while D is trained alternating on G and G2.
G has a standard BCE (binary cross entropy) loss based on D's rating.
G2 gets a mixture of a BCE loss on D's rating as well as a BCE loss on the difference between the images generated by G and G2.
The gradient from the second loss (image difference) is multiplied by a factor of `0.01`.
As a result, G2 is mostly trained to preserve G's images and - to a low amount - to generate images that D likes.
(Any another multiplier (lower/higher) led to worse results.)

After finishing the base training, the *coarse to fine* networks are trained.
One from 16 to 22px (height/width) and another one from 22 to 32px (so both roughly +50% size).
They follow the mentioned laplacian pyramid technique.
G gets fed blurry images and has to create sensible refinements for them (to get rid of the blurriness).
D gets fed blurry images with refinements that are either real (calculated based on the original images) or fake (generated by G).
The blurry images are created by downscaling an original image to coarse size (e.g. 16x16px) and then upsampling to fine size (e.g. 22x22px).
Both refinement pairs get images that are normalized to a range between -1.0 to +1.0.
(While the base 16x16 pair of G and D gets images with values in the range of 0.0 to +1.0).
Normalizing seemed necessary to get good results (might have been a quirk of the specific architectures).

When sampling images from the whole network, they are first generated by the 16x16 G.
Then D can be applied to filter out bad images.
The result is upscaled to 22x22px, normalized (-1.0 to +1.0) and gets fed into the first coarse to fine G.
G created a couple of possible refinements and D selects the best looking one.
The procedure is then repeated for 32x32px images.

`Adam` was used as the optimizer during training. Batch size was usually 32, which was split for D and G. So D would get 16 images (8 fake, 8 real) and G would get 16 tries.

![Training procedure overview](diagrams/training_procedure.png?raw=true "Training procedure overview")
